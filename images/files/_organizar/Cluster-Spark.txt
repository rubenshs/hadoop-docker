# Configurando um Cluster Spark

Spark é um sistema de computação em cluster de uso geral. Ele pode implementar e executar aplicativos paralelos em clusters, variando de um único nó a milhares de nós distribuídos. O Spark foi originalmente projetado para executar aplicativos Scala, mas também suporta Java, Python e R.

O Spark pode ser executado como um gerenciador de cluster independente ou aproveitando as estruturas de gerenciamento de cluster dedicadas, como Apache Hadoop YARN ou Apache Mesos.

O procedimento abaixo considera que você tem um cluster Hadoop configurado e em execução (com os serviços HDFS e YARN inicializados)


# Caso os serviços ainda não estejam iniciados, inicie-os:

$HADOOP_HOME/sbin/start-dfs.sh
$HADOOP_HOME/sbin/start-yarn.sh


# Logado como root, acesse o diretório /opt e faça download da versão 2.3 do Apache Spark.

OBS: O SPARK É INSTALADO APENAS NO NODE MASTER. O YARN FAZ A SERIALIZAÇÃO PARA OS OUTROS NODES DO CLUSTER.

cd /opt
wget http://www-us.apache.org/dist/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz
tar -xvf spark-2.3.0-bin-hadoop2.7.tgz
mv spark-2.3.0-bin-hadoop2.7 spark
chown -R hadoop:hadoop spark


# Logado como usuário Hadoop, adicione as variáveis de ambiente

cd ~
vi .bash_profile

# Spark 
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export SPARK_HOME=/opt/spark
export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH
export PATH=$PATH:$SPARK_HOME/bin

source .bash_profile

Para testar, digite: spark-shell. Se aparecer o shell do Spark sua variáveis estão ok. Digite :q para sair do shell.


# Renomeie o arquivo abaixo:

mv $SPARK_HOME/conf/spark-defaults.conf.template $SPARK_HOME/conf/spark-defaults.conf


# Edite o arquivo $SPARK_HOME/conf/spark-defaults.conf e acrescente a linha abaixo. Isso permitirá que o Spark tenha interação com o YARN.

spark.master    yarn


##############################################################################################################

Os jobs (processamento de dados) do Spark podem ser executados no YARN em dois modos: modo de cluster e modo de cliente. Entender a diferença entre os dois modos é importante para escolher uma configuração de alocação de memória apropriada e para enviar jobs conforme o esperado.

Um job do Spark consiste em duas partes: Executores do Spark que executam as tarefas reais e um Driver do Spark que programa os Executores.

Modo de cluster: tudo é executado dentro do cluster. Você pode iniciar um job em seu laptop e o job continuará em execução mesmo se você fechar o computador. Neste modo, o Spark Driver é encapsulado dentro do YARN Application Master.

No modo cliente, o driver Spark é executado em um cliente, como seu laptop. Se o cliente for desligado, o job falhará. Os Executores do Spark ainda são executados no cluster e, para agendar tudo, um pequeno YARN Application Master é criado.

O modo cliente é adequado para jobs interativos, mas os aplicativos falharão se o cliente parar. Para jobs de longa duração, o modo de cluster é mais apropriado.

A alocação de contêineres Spark para executar em contêineres YARN pode falhar se a alocação de memória não estiver configurada corretamente. Para nós com menos de 4G de RAM, a configuração padrão não é adequada e pode desencadear swap e baixo desempenho, ou até mesmo a falha de inicialização do aplicativo devido à falta de memória.

Certifique-se de entender como o Hadoop YARN gerencia a alocação de memória antes de editar as configurações de memória do Spark, para que as alterações sejam compatíveis com os limites do cluster do YARN. No script 04 da configuração do cluster Hadoop, adicionamos parâmetros de memória adicionais para permitir a execução de jobs Spark, mesmo em máquinas com pouca memória RAM.

##############################################################################################################

# Edite o arquivo $HADOOP_CONF_DIR/yarn-site.xml e certifique-se que o parâmetro yarn.scheduler.maximum-allocation-mb está com o valor 1536.


# Edite o arquivo $SPARK_HOME/conf/spark-defaults.conf e adicione o parâmetro abaixo para configurar a alocação de memória do Spark Driver em Cluster Mode.

spark.driver.memory    512m


# Edite o arquivo $SPARK_HOME/conf/spark-defaults.conf e adicione o parâmetro abaixo para configurar a alocação de memória do Spark Exxecutor:

spark.executor.memory          512m


##############################################################################################################

Estamos usando valores baixos de memória por conta das limitações de memória em nossas VMs. Em cluster de produção ajustamos esses parâmetros de acordo com a capacidade esperada para o cluster.

##############################################################################################################

# Estamos prontos para submeter um job Spark de processamento de dados sobre nosso cluster Hadoop. Execute um job com Spark-Submit. Esse job pode ser qualquer aplicação de processamento de dados. Neste exemplo, usamos o Spark examples. Após a execução você deve ver o status como SUCCEEDED!

spark-submit --deploy-mode cluster \
               --class org.apache.spark.examples.SparkPi \
               $SPARK_HOME/examples/jars/spark-examples_2.11-2.3.0.jar 10


##############################################################################################################

Quando você envia um job, o Spark Driver inicia automaticamente uma Web UI (User Interface) na porta 4040 que exibe informações sobre o aplicativo. No entanto, quando a execução é concluída, a interface com o usuário da Web é descartada com o driver do aplicativo e não pode mais ser acessada.

O Spark fornece um History Server que coleta logs do aplicativo do HDFS e os exibe em uma interface da Web persistente. As etapas a seguir ativarão a persistência de log no HDFS:

##############################################################################################################

# Edite $SPARK_HOME/conf/spark-defaults.conf e adicione as linhas abaixo para ativar os logs do Spark.

spark.eventLog.enabled  true
spark.eventLog.dir hdfs://hdpmaster:19000/spark-logs


# Crie um diretório no HDFS para os logs do Spark:

hdfs dfs -mkdir /spark-logs


# Ativando o History Server para analisar os logs de exxecução do Spark:

Edite $SPARK_HOME/conf/spark-defaults.conf e adicione as linhas abaixo:

spark.history.provider            org.apache.spark.deploy.history.FsHistoryProvider
spark.history.fs.logDirectory     hdfs://hdpmaster:19000/spark-logs
spark.history.fs.update.interval  10s
spark.history.ui.port             18080


# Inicie o History Server:

$SPARK_HOME/sbin/start-history-server.sh


# Exxecute novamente o Spark Job e agora você terá os logs de execução:

spark-submit --deploy-mode cluster \
               --class org.apache.spark.examples.SparkPi \
               $SPARK_HOME/examples/jars/spark-examples_2.11-2.3.0.jar 10


# Acesse o History Server via browser:

http://hdpmaster:18080


##############################################################################################################

O Spark Submit é usado para disparar um job no cluster de forma automática, mas você pode fazer o mesmo manualmente usando o shell do Spark e linguagem Scala

##############################################################################################################

# Inicie o shell do Spark:

spark-shell


# Execute cada linha abaixo para contar o número de linhas no arquivo:

var input = spark.read.textFile("/datasets/questoes.csv")
input.filter(line => line.length()>0).count()

--> Perceba que o nosso script Scala está fazendo a leitura de um arquivo no HDFS e processando em memória com o Spark!

Parabéns! Você completou mais uma etapa! Vem muito mais por aí nos próximos capítulos!


Obs: Caso tenha problemas de memória por conta das limitações da máquina virtual, adicione os 2 parâmetros abaixo no arquivo yarn-site.xml

yarn.nodemanager.vmem-check-enabled = false 
yarn.nodemanager.vmem-pmem-ratio = 4





